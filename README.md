# SONIVA: Speech recOgNItion Validation in Aphasia

This repository contains the code, models, and data developed for the **SONIVA project**.  
It includes **two main components**:

### 1. Acoustic Classification
We provide openSMILE-extracted acoustic features, along with machine learning models â€” **Support Vector Machine (SVM)**, **Random Forest (RF)**, and a **Neural Network (NN)** using focal loss and dropout regularization. These models are trained to classify speech segments as either **post-stroke (Patient)** or **healthy (Control)**, demonstrating the potential of acoustic biomarkers in clinical classification.

### 2. ASR Whisper Fine-Tuning
We release a **fine-tuned Whisper ASR model** trained on sentence-wise `.cha` transcripts derived from SONIVA recordings. These transcripts (but not raw audio) are included for benchmarking and adaptation, enabling the evaluation of **Word Error Rate (WER)** on post-stroke speech and supporting further research on clinical ASR performance.

---


## ğŸ“¥ Data Access

All SONIVA datasets (acoustic features, metadata, ASR model, and transcripts) are hosted together on **OneDrive** for secure sharing:

**[ğŸ”— Download SONIVA Data and Models](https://imperiallondon-my.sharepoint.com/shared?id=%2Fpersonal%2Ffg00%5Fic%5Fac%5Fuk%2FDocuments%2FSpeech%5FRecognition%5Fshared%2FSONIVA%2F%280%29%20SONIVA%5FFor%5FPublication%20%28GIULIA%29%2FSONIVA%5FPAPER&sortField=LinkFilename&isAscending=true)**

The OneDrive package includes:
- `acoustic_features_with_id.xlsx` â€“ Extracted acoustic features with subject IDs and labels.  
- `metadata.xlsx` â€“ Clinical and demographic metadata (IC3 and PLORAS columns).  
- `.cha` transcript files â€“ Orthographic and phonetic transcripts for ASR benchmarking.  
- Fine-tuned Whisper Medium model â€“ Fully trained ASR model ready for testing.

---

## ğŸ“‚ Repository Structure

```bash
SONIVA_PAPER/
â”œâ”€â”€ acoustic_classification/ # Acoustic feature classification (Experiment 1)
â”‚ â”œâ”€â”€ main.py
â”‚ â”œâ”€â”€ requirements.txt
â”‚ â”œâ”€â”€ README_acoustic.md
â”‚ â””â”€â”€ data/
â”‚ â””â”€â”€ README.md # Points to OneDrive dataset
â”‚
â”œâ”€â”€ asr_whisper_finetuning/ # Whisper ASR fine-tuning + transcripts (Experiment 2)
â”‚ â”œâ”€â”€ README_asr.md
â”‚ â”œâ”€â”€ download_model.sh
â”‚ â””â”€â”€ transcripts/
â”‚ â””â”€â”€ README.md # Points to CHA files
â”‚
â”œâ”€â”€ DATA_ACCESS.md
â”œâ”€â”€ README.md # This file
â”œâ”€â”€ setup.sh
â”œâ”€â”€ .gitignore
â””â”€â”€ .github/
â””â”€â”€ workflows/
â””â”€â”€ test.yml

```


---

## ğŸš€ Quick Start

### Acoustic Classification
1. Download the acoustic features from OneDrive and place them in `acoustic_classification/data/`.
2. Install dependencies and run:
   ```bash
   cd acoustic_classification
   pip install -r requirements.txt
   python main.py --data_path data/acoustic_features_with_id.xlsx --model all
   ```

## ASR Whisper Fine-Tuning

1. **Download the fine-tuned Whisper model and `.cha` transcripts** from OneDrive:  
   [ğŸ”— Download ASR Model + Transcripts](PUT_ONEDRIVE_LINK_ASR_HERE)

2. **Follow the detailed instructions** provided in [README_asr.md](asr_whisper_finetuning/README_asr.md).


## ğŸ“š Citation
If you use this code or dataset in your research, please cite our paper:
```bibtex
@article{sanguedolce2025soniva,
  ...
}
```
---

## ğŸ™ Acknowledgments
- [Funding sources]
- [Collaborators]

---

## ğŸ“ Contact
For questions about this code or paper:
- **First Author:** gs2022@ic.ac.uk
- **Corresponding Author:** fatemeh.geranmayeh00@imperial.ac.uk
- **Issues:** Please use GitHub Issues for technical problems

---

**Last Updated:** 29/07/2025  
**Version:** 1.0.0


